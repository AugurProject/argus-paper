%
% File ACL2016.tex
%

\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{acl2016}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}


\title{Joint Learning of Sentence Embeddings for Relevance and Entailment}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
\author{Petr Baudi\v{s},
	Silvestr Stanko \and
	Jan \v{S}ediv\'{y} \\
	FEE CTU Prague\\
	Department of Cybernetics\\
	Technick\'{a} 2, Prague,\\Czech Republic\\
	{\tt baudipet@fel.cvut.cz}}

%\author{Author 1\\
%	    XYZ Company\\
%	    111 Anywhere Street\\
%	    Mytown, NY 10000, USA\\
%	    {\tt author1@xyz.org}
%	  \And
%	Author 2\\
%  	ABC University\\
%  	900 Main Street\\
%  	Ourcity, PQ, Canada A1A 1T2\\
%  {\tt author2@abc.ca}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
	We consider the problem of Recognizing Textual Entailment
	within an Information Retrieval context, where we must simultaneously
	determine the relevancy as well as degree of entailment for individual
	pieces of evidence to determine a yes/no answer to a binary
	natural language question.

	We introduce an open dataset of questions and evidence, without
	explicit evidence supervision, that we use to train a neural model
	that produces sentence embeddings which are suitable for classifying
	evidence of varying relevance.  We show that joint training of
	relevance and entailment is possible and that our proposed model
	is powerful enough to correctly answer most questions even when
	given a relatively small dataset, even in the context of
	strong non-neural IR baselines.
\end{abstract}

\section{Introduction}

Let us consider the goal of building machine reasoning systems based
on knowledge from fulltext data like encyclopedic articles, scientific
papers or news articles.
Such machine reasoning systems, like humans researching a problem,
must be able to retrieve evidence from large amounts of retrieved
but irrelevant information and judge what answer the evidence entails
to the question at hand.

A typical approach, used implicitly in information retrieval
(and its extensions, like IR-based Question Answering systems \cite{YodaQAPoster2015}),
is to determine evidence relevancy by a keyword overlap feature (like tf-idf or BM-25 \cite{BM25})
and prune the evidence by the relevancy score.
On the other hand, textual entialment systems that seek to confirm hypotheses
based on evidence \cite{RTE1} \cite{SICK2014} \cite{SNLI}
are typically provided with only a single piece of evidence
or only evidence pre-determined as relevant, and are often restricted
to short and simple sentences without open-domain named entity occurences.

In this work, we seek to fuse information retrieval and textual entaiment
recognition by defining the \textbf{Argus Yes/No Question Answering} task.
The problem is, given a real-world event binary question like
\textit{Did Donald Trump announce he is running for president?}
and numerous retrieved news article fragments as evidence,
to determine the answer for the question.

This problem is motivated by the Argus automatic reporting system for
the Augur prediction market paltform. \cite{argus}  Therefore, we consider the question answering task
within the constraints of a practical task that has limited available dataset
and only minimum supervision.  Hence, the evidence are authentic news
sentences (with noise like segmentation errors, irrelevant participial phrases, etc.),
and whereas we have gold standard for
the correct answers, the model must do without explicit supervision on
which individual evidence snippets are relevant and what do they entail.

To this end, we introduce an open dataset of questions and newspaper evidence,
and a neural model within the Sentence Pair Scoring framework \cite{sps}
that (A) learns sentence embeddings for the question and evidence,
(B) the embeddings represent both relevance and entailment characteristics
as linear classifier inputs, and (C) the model aggregates all available evidence
to produce a binary signal as the answer, which is the only training supervision.

The paper is structured as follows.  In Sec.~\ref{sec:argus}, we formally outline
the Argus question answering task and describe the question-evidence dataset.
In Sec.~\ref{sec:relwork}, we briefly survey the related work on similar problems,
whereas in Sec.~\ref{sec:clasrel} we present our ClasRel neural model for joint
learning of sentence relevance and entailment.  We present the results in Sec.~\ref{sec:res}
and conclude with a summary and future work directions in Sec.~\ref{sec:concl}.

\section{The Argus Task}
\label{sec:argus}

We propose a solution to the Argus Task, where
the \textbf{Argus} system \cite{arguswp} \cite{argus}
is to automatically analyze and answer questions
in the context of the \textbf{Augur} prediction market platform.%
\footnote{\url{https://augur.net/}}
In a prediction market, some users pose questions about the future
whereas other users bet on the \textit{yes} or \textit{no} answer,
with the assumption that the bet price reflects the real probability
of the event.  At a specified moment (e.g.\ after the date of a to-be-predicted sports match), the
correct answer is retroactively determined and the bets are paid off.
At a larger volume of questions, determining the bet results may
present a significant overhead for running of the market.
This motivates the Argus system, which should partially automate
this determination --- deciding questions related to recent events
based on open news sources.

Formally, the Argus task is to build a function $y_i = f_h(H_i)$,
where $y_i \in [0,1]$ is a binary label (\textit{no} towards \textit{yes})
and $H_i = (q_i, M_i)$ is a hypothesis in the form of question text $q_i$
and a set of $M_i = \{m_{ij}\}$ memory texts $m_{ij}$ as extracted
from an evidence-carrying corpus.

TODO: example


\subsection{Dataset}

To help develop a machine learning model for the $f_h$ function,
we have created a dataset of questions with gold labels, and used
this dataset with an IR information retrieval component of the Argus
system to generate memory texts from a variety of news papers.
We release this dataset openly.%
\footnote{\url{https://github.com/brmson/dataset-sts} directory data/hypev/argus}

To pose a reproducible task for the IR component, the time domain
of questions was restricted from September 1, 2014 to September 1, 2015,
and topic domain was focused to politics, sports and the stock market.
To build the question dataset, we have used several sources:
\begin{itemize}
	\item We asked Amazon Mechanical Turk users to pose questions, together with a golden label and a news article reference.
		This seeded the dataset with initial, somewhat repetitive 250 questions.
	\item We manually extended this dataset by derived questions with reversed polarity (to obtain an opposite answer).
	\item We extended the data with questions autogenerated from templates, pertaining top sporting event winners and US senate or gubernatorial elections.
\end{itemize}

To build the memories dataset, we used the \textbf{Syphon} component
\cite{argus}
of the stock Argus implementation%
\footnote{\url{https://github.com/AugurProject/argus}}
to identify semantic roles of all question tokens and produce
the search keywords if a role was assigned to each token.
We then used the stock IR component to query a corpus of newspaper
articles, and kept sentences that contained at least 2/3 of all
the keywords.
Our corpus of articles contained articles from The Guardian (all articles) and from the New York Times (Sports, Politics and Business sections).  Furthermore, we scraped partial archive.org historical data out of 35 RSS feeds from CNN, Reuters, BBC International, CBS News, ABC News, c|net, Financial Times, Skynews and the Washington Post.

For the final dataset, we kept only questions where at least
a single memory was found (i.e.\ we successfuly assigned a role
to each token, found some news stories and found at least one
sentence with 2/3 of question keywords within).  The final size
of the dataset is outlined in Fig.~\ref{tab:dataset}.

\begin{figure}
	\centering
	\begin{tabular}{|c|ccc|}
		\hline
		& Train & Val. & Test \\
		\hline
		Original $\#q$ & 1829 & 303 & 295 \\
		Post-search $\#q$ & 1081 & 167 & 158 \\
		Average $\#m$ per q. & 19.04 & 13.99 & 16.66 \\
		\hline
	\end{tabular}
	\vspace*{-0.2cm}
	\caption{\footnotesize%
		Characteristics of the Argus QA dataset.
	}
	\label{tab:dataset}
\end{figure}


\section{Related Work}
\label{sec:relwork}

Our primary concern when integrating natural language query and
textual evidence is to build sentence-level representations that
can be used both for relevance weighing and answer prediction.

Sentence-level representations in the retrieval + inference context have been
popularly proposed within the Memory Network framework \cite{MemNN},
but just averaged word embeddings are explored; the task includes
only very simple sentences and a small vocabulary.
Much more realistic setting is introduced in the Answer Sentence Selection
context \cite{AnsselWang} \cite{sps}, with state-of-art models using complex
deep neural architectures with attention \cite{attnpooling}, but ths selection
task consists of only retrieval and no inference (answer prediction).
A more indirect retrieval task regarding news summarization was investigated
by \cite{AttSum}.

In the entailment context, \cite{SNLI} introduced a large dataset
with single-evidence sentence pairs (Stanford Natural Language Inference, SNLI),
but a larger vocabulary and
slightly more complicated (but still conservatively formed) sentences
They also proposed baseline recurrent neural model for modeling
sentence representations, while word-level attention based models
are studied more recently \cite{SNLIattn} \cite{LSTMMR}.

TODO SICK-2014

The most similar task to the binary question answering we consider
is the MCTest text comprehension challenge \cite{MCTest}, where
users are presented with short stories and are asked to answer multiple
choice questions about details of these stories.  The state-of-art model
of \cite{HABCNN} uses convolutional neural networks to build sentence
representations, and attention on multiple levels to select evidencing
sentences.
However, the practical setting differs as the MCTest dataset contains
relatively restricted vocabulary and well-formed sentences; furthermore,
the goal is to find the single key point in the story to focus on, while
in our setting we may have many pieces of evidence supporting an answer.

\section{ClasRel Neural Model}
\label{sec:clasrel}

\section{Results}
\label{sec:res}

\subsection{Experimental Setup}

\subsection{Evaluation}

Rnn features only:
Train: 0.936168 ±0.014544
Val: 0.883608 ±0.008952
Test: 0.829114 ±0.011312

Avg features only:
Train: 0.917400 ±0.003061
Val: 0.768713 ±0.009674
Test: 0.678006 ±0.009330

cnn features only:
Train: 0.959917 ±0.009605
Val: 0.880988 ±0.014256
Test: 0.821994 ±0.020737

attn1511 features only:
Train: 0.936399 ±0.011737
Val: 0.899701 ±0.008580
Test: 0.854430 ±0.009165

Average end-to-end: argus tests, commit 0addc0d - 3d input, na konci mean
classification (uniform relevance), classification by rnn:
Train: 0.730074 ±0.008370
Val: 0.729042 ±0.006503
Test: 0.674842 ±0.012944

relevance by BM25:
TODO

\subsection{Embeddings Analysis}

\section{Conclusion}
\label{sec:concl}


\section*{Acknowledgments}
{\footnotesize
	This work was co-funded by the Augur Project of the Forecast Foundation
and financially supported by the Grant Agency of the Czech Technical
University in Prague, grant No. SGS16/ 084/OHK3/1T/13.
Computational resources were provided by the CESNET LM2015042 and the CERIT Scientific Cloud LM2015085,
provided under the programme ``Projects of Large Research, Development, and Innovations Infrastructures.''

We'd like to thank Peronet Despeignes of the Augur Project for his support.}

\bibliography{qa,sps,argus}
\bibliographystyle{acl2016}

\end{document}
